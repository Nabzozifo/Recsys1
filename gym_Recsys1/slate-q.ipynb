{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Recsys1_env'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92cca15d9c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gym_Recsys1/gym_Recsys1/envs/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mRecsys1_env\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Recsys1_env'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import itertools \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys \n",
    "from collections import defaultdict \n",
    "import plottings  \n",
    "matplotlib.style.use('ggplot') \n",
    "import gym_Recsys1\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('gym_Recsys1/gym_Recsys1/envs/')\n",
    "import Recsys1_env as rcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions): \n",
    "\t\t\"\"\" \n",
    "\t\tCreates an epsilon-greedy policy based \n",
    "\t\ton a given Q-function and epsilon. \n",
    "\t\t\n",
    "\t\tReturns a function that takes the state \n",
    "\t\tas an input and returns the probabilities \n",
    "\t\tfor each action in the form of a numpy array  \n",
    "\t\tof length of the action space(set of possible actions). \n",
    "\t\t\"\"\"\n",
    "\t\tdef policyFunction(state): \n",
    "\t\n",
    "\t\t\tAction_probabilities = np.ones(num_actions, \n",
    "\t\t\t\t\tdtype = float) * epsilon / num_actions \n",
    "\t\t\t\t\t\n",
    "\t\t\tbest_action = np.argmax(Q[state]) \n",
    "\t\t\tAction_probabilities[best_action] += (1.0 - epsilon) \n",
    "\t\t\treturn Action_probabilities \n",
    "\t\n",
    "\t\treturn policyFunction\n",
    "\n",
    "\tdef qLearning(env, num_episodes, discount_factor = 1.0, \n",
    "\t\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1): \n",
    "\t\t\"\"\" \n",
    "\t\tQ-Learning algorithm: Off-policy TD control. \n",
    "\t\tFinds the optimal greedy policy while improving \n",
    "\t\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\t\t\n",
    "\t\t# Action value function \n",
    "\t\t# A nested dictionary that maps \n",
    "\t\t# state -> (action -> action-value). \n",
    "\t\tQ = defaultdict(lambda: np.zeros(env.action_space.n*4)) \n",
    "\n",
    "\t\t# Keeps track of useful statistics \n",
    "\t\tstats = plottings.EpisodeStats( \n",
    "\t\t\tepisode_lengths = np.zeros(num_episodes), \n",
    "\t\t\tepisode_rewards = np.zeros(num_episodes))\t \n",
    "\t\t\n",
    "\t\t# Create an epsilon greedy policy function \n",
    "\t\t# appropriately for environment action space \n",
    "\t\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n*4) \n",
    "\t\t\n",
    "\t\t# For every episode \n",
    "\t\tfor ith_episode in range(num_episodes): \n",
    "\t\t\tstate = env.reset()\n",
    "\t\t\t# Reset the environment and pick the first action \n",
    "\t\t\tfor t in itertools.count(): \n",
    "\t\t\t\t\n",
    "\t\t\t\t# get probabilities of all actions from current state \n",
    "\t\t\t\taction_probabilities = policy(state) \n",
    "\n",
    "\t\t\t\t# choose action according to \n",
    "\t\t\t\t# the probability distribution \n",
    "\t\t\t\taction = np.random.choice(np.arange( \n",
    "\t\t\t\t\t\tlen(action_probabilities)), \n",
    "\t\t\t\t\t\tp = action_probabilities) \n",
    "\n",
    "\t\t\t\t# take action and get reward, transit to next state \n",
    "\t\t\t\tnext_state, reward, done, _ = env.step(action) \n",
    "\n",
    "\t\t\t\t# Update statistics \n",
    "\t\t\t\tstats.episode_rewards[ith_episode] += reward \n",
    "\t\t\t\tstats.episode_lengths[ith_episode] = t \n",
    "\t\t\t\t\n",
    "\t\t\t\t# TD Update \n",
    "\t\t\t\t\n",
    "\t\t\t\tbest_next_action = np.argmax(Q[next_state])\t \n",
    "\t\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action] \n",
    "\t\t\t\ttd_delta = td_target - Q[state][action] \n",
    "\t\t\t\tQ[state][action] += alpha * td_delta \n",
    "\n",
    "\t\t\t\t# done is True if episode terminated \n",
    "\t\t\t\tif done: \n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tstate = next_state \n",
    "\t\t\n",
    "\t\treturn Q,stats"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38232bitf3d33b7dbdfd4b3b9bc8a649876e77c1",
   "display_name": "Python 3.8.2 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}